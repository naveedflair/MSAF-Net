# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rDV1EceVBQpoacWnWM3LKJkYH9Rq5Qwz
"""

import torch.nn as nn

class AdaptiveAttention(nn.Module):
    def __init__(self, in_channels):
        super(AdaptiveAttention, self).__init__()
        self.conv_downsample = nn.Conv2d(in_channels, 64, kernel_size=1)
        self.conv = nn.Conv2d(64, 8, kernel_size=1)
        self.conv2 = nn.Conv2d(8, in_channels, kernel_size=1)

    def forward(self, x):
        x = self.conv_downsample(x)
        x = self.conv(x)
        x = torch.sigmoid(self.conv2(x))
        return x

class ContextualIntegration(nn.Module):
    def __init__(self, in_channels):
        super(ContextualIntegration, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, context):
        context_conv = self.conv1(context)
        gating_signal = self.sigmoid(self.conv2(context_conv))
        out = x * gating_signal
        return out