# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rDV1EceVBQpoacWnWM3LKJkYH9Rq5Qwz
"""

import torch.nn as nn
import torch.nn.functional as F
from .fpn import FPN
from .attention import AdaptiveAttention, ContextualIntegration

class MSAF_Net(nn.Module):
    def __init__(self, backbone):
        super(MSAF_Net, self).__init__()
        self.backbone = backbone
        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.backbone.bn1 = nn.BatchNorm2d(64)
        self.backbone.relu = nn.ReLU(inplace=True)
        self.backbone.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.fpn = FPN(self.backbone)
        self.attention = nn.ModuleList([AdaptiveAttention(in_channels=256) for _ in range(4)])
        self.context_integration = ContextualIntegration(in_channels=256)
        self.classifier = nn.Conv2d(256, 1, kernel_size=1)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        input_size = x.size()[2:]

        x = self.backbone.conv1(x)
        x = self.backbone.bn1(x)
        x = self.backbone.relu(x)
        x = self.backbone.maxpool(x)

        features = self.fpn(x)

        target_size = features[0].size()[2:]
        resized_features = [F.interpolate(feature, size=target_size, mode='bilinear', align_corners=False)
                           for feature in features]

        attention_features = []
        for i, feature in enumerate(resized_features):
            attention_feature = self.attention[i](feature)
            attention_features.append(attention_feature)

        context = sum(attention_features)
        refined_features = []
        for feature in attention_features:
            refined_feature = self.context_integration(feature, context)
            refined_features.append(refined_feature)

        x = self.dropout(sum(refined_features))
        output = self.classifier(x)
        output = F.interpolate(output, size=input_size, mode='bilinear', align_corners=False)
        return torch.sigmoid(output) * 0.98 + 0.01